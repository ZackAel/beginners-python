{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZackAel/beginners-python/blob/master/moe-f-blanka.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXKX_y99Uso_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "## --- Setup LOGGING --- ##\n",
        "from datetime import datetime\n",
        "\n",
        "import fire\n",
        "import torch.nn.functional as _F\n",
        "from loguru import logger\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Log DEBUG to .log file:\n",
        "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "base_name = os.path.basename(__file__).split(\".\")[0]\n",
        "\n",
        "log_dir = os.path.join(script_dir, \"logs\")\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "log_path = os.path.join(log_dir, f\"{base_name}_{timestamp}.log\")\n",
        "logger.add(log_path,\n",
        "           format=\"{time} {level} {message}\",\n",
        "           level=\"DEBUG\",\n",
        "           filter=lambda record: record[\"level\"].name == \"DEBUG\",\n",
        "           mode='w'\n",
        ")\n",
        "\n",
        "## --- CONSTANTS --- ###\n",
        "EPSILON = 1e-5\n",
        "\n",
        "## --- HELPERS --- ###\n",
        "from utils_common.utils_common import *\n",
        "from utils_mse import utils_moef as um\n",
        "\n",
        "## -- MSE HELPER FUNCTIONS -- ##\n",
        "def compute_A_mse(\n",
        "        w_i: torch.Tensor,\n",
        "        y_t: torch.Tensor,\n",
        "        X_until_t: torch.Tensor,\n",
        "        experts: list,\n",
        "        delta: float,\n",
        "        t: int,\n",
        "        F: torch.Tensor = None,\n",
        "        DeltaF: torch.Tensor = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Compute the matrix A, which represents the adjusted predictions of experts.\n",
        "    :return: A tensor representing the A matrix with shape (N,).\n",
        "    \"\"\"\n",
        "    N = len(experts)\n",
        "    if F is None:\n",
        "        #F = torch.tensor([expert(X_until_t) for expert in experts], dtype=torch.float)\n",
        "        F = torch.stack([expert(X_until_t) for expert in experts], dim = 0)\n",
        "    if DeltaF is None:\n",
        "        if X_until_t.shape[0] > 1:\n",
        "            #F_prior = torch.tensor([expert(X_until_t[:-1]) for expert in experts], dtype=torch.float)\n",
        "            F_prior = torch.stack( [expert(X_until_t[:-1] ) for expert in experts], dim=0 )\n",
        "        else:\n",
        "            F_prior = torch.zeros_like(F, dtype=torch.float)  # F : (N, H*C)\n",
        "        DeltaF = F - F_prior\n",
        "\n",
        "    F = F.float()  # Redundant for MSE case with F -> (N, H*C)\n",
        "    wF = torch.sum(w_i.view(-1, 1) * F, dim=0)\n",
        "    LHS = (y_t - F)\n",
        "    RHS = wF - DeltaF + 1\n",
        "    A = 2 * LHS * RHS\n",
        "\n",
        "    return A\n",
        "\n",
        "def compute_B_mse(\n",
        "        Y: torch.Tensor,\n",
        "        x: torch.Tensor,\n",
        "        experts: list,\n",
        "        delta: float,\n",
        "        t: int,\n",
        "        F: torch.Tensor = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute the matrix B, which scales the error terms.\n",
        "    :return: A tensor representing the B matrix with shape (N,).\n",
        "    \"\"\"\n",
        "    if F is None:\n",
        "        F = torch.stack([expert(x) for expert in experts], dim=0)\n",
        "    #B = 2 ** (3 / 2) * torch.exp(t * 4 * log_delta) * (Y - F)\n",
        "    B = 2 ** (3 / 2) * (Y - F)\n",
        "    return B\n",
        "\n",
        "def compute_ell_mse(\n",
        "        Y: torch.Tensor,\n",
        "        X_until_t: torch.Tensor,\n",
        "        experts: list,\n",
        "        F: torch.Tensor = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute the loss for each expert's prediction.\n",
        "    :return: A tensor representing the loss matrix with shape (N,).\n",
        "    \"\"\"\n",
        "    loss_func = torch.nn.MSELoss()\n",
        "    if F is None:\n",
        "        F = torch.stack([expert(X_until_t) for expert in experts], dim=0)\n",
        "\n",
        "    ell = torch.tensor([loss_func(f, Y) for f in F])\n",
        "    return ell\n",
        "\n",
        "## -- END - MSE HELPER FUNCTIONS -- ##\n",
        "\n",
        "def compute_A_bar(\n",
        "        pi: torch.Tensor,\n",
        "        y_t: torch.Tensor,\n",
        "        X_until_t: torch.Tensor,\n",
        "        experts: list,\n",
        "        delta: float,\n",
        "        t: int,\n",
        "        F: torch.Tensor = None,\n",
        "        DeltaF: torch.Tensor = None,\n",
        "        use_uniform_w: bool = False,\n",
        "        normalize_A: bool = True,\n",
        "        *args, **configs\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        Compute the weighted sum of the matrix A across all experts.\n",
        "        :param pi: Expert weights with shape (N, N).\n",
        "        :param y_t: (H*C)\n",
        "        :param X_until_t: Input features with shape (t, N, H*C ).\n",
        "        :param experts: A list of expert functions.\n",
        "        :param delta: A float value for adjustments.\n",
        "        :param t: Current time step as an integer.\n",
        "        :param F: Expert predictions\n",
        "        :param DeltaF:\n",
        "        :param use_uniform_w: Should be always False, Boolean to decide whether to use uniform weights.\n",
        "        :param normalize_A: Boolean to decide whether to normalize A.\n",
        "\n",
        "        :return: A tensor representing the weighted A matrix with shape (N,).\n",
        "    \"\"\"\n",
        "    is_using_mse = configs.get('is_using_mse', True)\n",
        "    delta = delta or configs.get('delta_val', 1)\n",
        "\n",
        "    N = len(experts)\n",
        "    if use_uniform_w:\n",
        "        A = torch.stack([compute_A_mse(torch.eye(N)[i], y_t, X_until_t, experts, delta, t, F, DeltaF,) for i in range(N)])\n",
        "    else:\n",
        "        A = torch.stack([compute_A_mse(w_i=pi[i],\n",
        "                               y_t=y_t,\n",
        "                               X_until_t=X_until_t,\n",
        "                               experts=experts,\n",
        "                               delta=delta,\n",
        "                               t=t, F=F, DeltaF=DeltaF)\n",
        "                                    for i in range(N)],\n",
        "                        dim=0)      # A.shape = (N, N, H*C) or (3, 3, 672)\n",
        "\n",
        "    if normalize_A:\n",
        "        row_sums = A.sum(dim=1, keepdim=True)\n",
        "        row_sums[row_sums == 0] = 1\n",
        "        A_normalized = A / row_sums\n",
        "        A_bar = torch.sum(A_normalized * pi.T.unsqueeze(-1), dim=0)  # A_normalized: (N, N, H*C); pi.t().unsqueeze(-1): (N, N, 1)\n",
        "    else:\n",
        "        A_bar = torch.sum(A * pi.T.unsqueeze(-1), dim=0)\n",
        "\n",
        "    return A_bar\n",
        "\n",
        "\n",
        "def mixture_of_filters(X: torch.Tensor, Y: torch.Tensor, seed: int, experts: list,\n",
        "                       lam: float,\n",
        "                       delta: float,\n",
        "                       *args, **configs) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Combine predictions from multiple experts using a dynamic weighting algorithm.\n",
        "    :param X: Input data tensor with shape (T, N, H, C). where T is the time in hours or rows\n",
        "                and each column 0:N-1 are experts pred. for day t in T.\n",
        "    :param Y: Output data tensor with shape (T, H, C).\n",
        "    :return: A tensor representing the predicted values.\n",
        "    \"\"\"\n",
        "    lam = lam or configs.get('lambda_val', 1.0)\n",
        "    delta = delta or configs.get('delta_val', 1)\n",
        "    update_Q = configs.get('update_Q', True)\n",
        "    normalize_pi = configs.get('normalize_pi', False)\n",
        "    is_log_values = configs.get('is_log_values', False)\n",
        "    clamp_at = configs.get('clamp_at', CLAMP_AT)\n",
        "\n",
        "    T = configs.get('T', 2785)\n",
        "    L = configs.get('L', 720)\n",
        "    H = configs.get('H', 96)\n",
        "    C = configs.get('C', 7)\n",
        "    N = len(experts)\n",
        "    assert X.shape == (T, N, H*C)\n",
        "    assert Y.shape == (T, H*C)\n",
        "\n",
        "    pi = torch.ones((N, N), dtype=torch.float) / N\n",
        "    Q = torch.ones((N, N)) / (N - 1) - torch.eye(N) / (N - 1) - torch.eye(N)\n",
        "    L_prior = torch.zeros(N)\n",
        "    F_prior = torch.zeros((N, H * C), dtype=torch.float)\n",
        "    next_pi = torch.zeros_like(pi, dtype=torch.float)\n",
        "    next_Q = torch.zeros_like(Q)\n",
        "    Y_hat = torch.zeros_like(Y)\n",
        "    prob_Y_hat = torch.zeros((T, H * C), dtype=torch.float)  # update at each iteration\n",
        "    PI_BAR_LIST = []\n",
        "    for t in tqdm(range(T)):\n",
        "        F = torch.stack([expert(X[:t + 1]) for expert in experts], dim=0)       # (3, 672)\n",
        "        DeltaF = F - F_prior\n",
        "        F_prior = F\n",
        "        X_until_t = X[:t+1]\n",
        "        A_bar = compute_A_bar(pi=pi, y_t=Y[t], X_until_t=X_until_t, experts=experts, delta=delta, t=t, F=F,\n",
        "                              DeltaF=DeltaF, *args, **configs)  # A_bar: (N,) i.e. (5,)\n",
        "        if torch.eq(A_bar, 0).any():\n",
        "            logger.warning(f\"WARNING: POTENTIAL 0 DIV: 0 value in A_bar at {t}\")\n",
        "            A_bar = A_bar + (A_bar == 0).float() * EPSILON\n",
        "            logger.info(f\"To A_bar(after): {A_bar} by adding {EPSILON} for numerical stability\")\n",
        "\n",
        "        B = compute_B_mse(Y=Y[t], x=X_until_t, experts=experts, delta=delta, t=t, F=F,)\n",
        "        if torch.isnan(B).any():\n",
        "            logger.warning(f\"ERROR: BUG: NaN value in B at {t}\")\n",
        "        if torch.eq(B, 0).any():\n",
        "            logger.warning(f\"WARNING: POTENTIAL 0 DIV: 0 value in B at {t}, htat will cause 0 division resulting in 'inf'\")\n",
        "            logger.info(f\"Adjusting B (before): {B}\")\n",
        "            B = B + (B==0).float() * EPSILON\n",
        "            logger.info(f\"To B(after): {B} by adding {EPSILON} for numerical stability\")\n",
        "\n",
        "        L = compute_ell_mse(Y=Y[t], X_until_t=X_until_t, experts=experts, F=F,)\n",
        "        if torch.isnan(L).any():\n",
        "            logger.warning(f\"ERROR: BUG: NaN value in L at {t} with seed: {seed}\")\n",
        "        DeltaL = L - L_prior\n",
        "        DeltaL = DeltaL.unsqueeze(-1)  # # Reshape DeltaL to be compatible with A_bar and B: (N,) -> (N, 1)\n",
        "        L_prior = L\n",
        "        Delta_W_bar = (DeltaL - A_bar) / B\n",
        "        As = torch.stack([compute_A_mse(w_i=torch.eye(N)[i],\n",
        "                                    y_t=Y[t],\n",
        "                                    X_until_t=X_until_t,\n",
        "                                    experts=experts, delta=delta, t=t, F=F,\n",
        "                                    DeltaF=DeltaF, )\n",
        "                            for i in range(N)]\n",
        "        )\n",
        "        check_for_nans([pi, Q, next_pi, F, As, B, L],\n",
        "                       [\"pi\", \"Q\", \"next_pi\", \"F\", \"As\" , \"B\", \"L\"], t, seed)\n",
        "        log_args = {\n",
        "            f\"Q_{t}_before_update\": Q.clone().detach(),\n",
        "            f\"pi_{t}_before_update\": pi.clone().detach(),\n",
        "            f\"L_{t}\": L.clone().detach(),\n",
        "            'DeltaL': DeltaL.clone().detach(),\n",
        "        }\n",
        "        for n in range(N):\n",
        "            pi_n = pi[n].unsqueeze(-1)          # (N, 1)\n",
        "            drift = Q @ pi_n\n",
        "            diffusion = pi_n * (As[:, n] - A_bar[n]) / B[n]\n",
        "            update = drift + diffusion * (Delta_W_bar[n] + EPSILON)\n",
        "            def get_normalized_z_score(u_mu):\n",
        "                _mean = u_mu.mean()\n",
        "                _std = u_mu.std()\n",
        "                _normalized = (u_mu - _mean) / _std\n",
        "                return _normalized\n",
        "            # Compute update_mean and normalize\n",
        "            update_mean = torch.mean(update, dim=-1)\n",
        "            update_normalized = get_normalized_z_score(update_mean)\n",
        "            pi_n_updated = pi_n.squeeze() + update_normalized.squeeze()\n",
        "            pi_n_updated = torch.nn.functional.softmax(pi_n_updated, dim=0) # Valid Prob. dist: Normalize to ensure it sums to 1\n",
        "            log_args.update({\n",
        "                f\"pi_update_{t}_{n}\": pi_n_updated\n",
        "            })\n",
        "\n",
        "            next_pi[n] = pi_n_updated\n",
        "            next_pi[n] = torch.clamp(next_pi[n], min=0)\n",
        "\n",
        "            if torch.isnan(next_pi[n]).any():\n",
        "                logger.warning(f\"ERROR: BUG: NaN value in next_pi[{n}] at {t}\")\n",
        "            if normalize_pi:\n",
        "                sum_next_pi_n = torch.sum(next_pi[n])\n",
        "                if sum_next_pi_n > 0:\n",
        "                    next_pi[n] = next_pi[n] / sum_next_pi_n\n",
        "                else:\n",
        "                    is_zerolike = torch.isclose(sum_next_pi_n, torch.tensor(0.0))\n",
        "                    logger.warning(\n",
        "                        f\"Sum of probabilities for next_pi[{n}] was zero at time {t}. Assigning equal probabilities.\")\n",
        "                    next_pi[n] = torch.full_like(next_pi[n], fill_value=1/len(next_pi[n]))\n",
        "            next_pi[n] = torch.clamp(next_pi[n], EPSILON, 1.0)  # Clamp values if necessary (adjust range as needed)\n",
        "\n",
        "        pi = next_pi.clone().detach()\n",
        "        loss = torch.nn.MSELoss()\n",
        "        s = s_mse = torch.tensor([loss(f, Y[t]) for f in (pi @ F)])\n",
        "        Y_hat_t = pi @ F\n",
        "        pi_bar_t = _F.softmin(lam*s, dim=0)   # (N, 1)\n",
        "\n",
        "        PI_BAR_LIST.append(pi_bar_t)    # Add to PI_BAR_LIST for plotting weight composition\n",
        "        Y_moef_t = pi_bar_t @ Y_hat_t\n",
        "        Y_hat[t] = Y_moef_t\n",
        "        log_args.update({\n",
        "            f\"s_mse_{t}\": s_mse.clone().detach(),\n",
        "            f\"pi_bar_{t}\": pi_bar_t.clone().detach(),\n",
        "        })\n",
        "        ## Ln 26-32: Update Q\n",
        "        if update_Q:\n",
        "            import scipy\n",
        "            alpha = configs.get(\"alpha_val\", 0.10)    # N.B. lowering Identity dominance (<0.10) may lead to numerical instability (complex parts)\n",
        "            N = pi_bar_t.shape[0]\n",
        "            P_tilde = pi_bar_t.repeat(N, 1)\n",
        "            P = (1 - alpha) * P_tilde + alpha * torch.eye(N)\n",
        "            P_eigvals = torch.linalg.eigvals(P)\n",
        "            log_P = torch.tensor(scipy.linalg.logm(P.numpy()).real, dtype=torch.float32)\n",
        "            M  = _F.relu(log_P)\n",
        "            RHS = torch.sum(M, dim=0, keepdim=True) * torch.eye(N)\n",
        "            Q  = M - RHS\n",
        "\n",
        "        if is_log_values:\n",
        "            log_values(t, **log_args)\n",
        "        logger.info(\"-\" * 25)\n",
        "        logger.info(f\"==== Run for t:{t} completed ====\")\n",
        "        logger.info(\"-\" * 25)\n",
        "    # End Loop For 1...T\n",
        "    return Y_hat, None, Q, pi, PI_BAR_LIST\n",
        "\n",
        "\n",
        "def run_experiments_data(id: str, seed: int, experiments_dir: str = EXPERIMENTS_PATH, **configs):\n",
        "    is_save_results = configs.get('is_save_results', True)\n",
        "    lambda_val = configs.get('lambda_val', 1.0)\n",
        "    delta_val = configs.get('delta_val', 1)\n",
        "    alpha_val = configs.get('alpha_val', 0.10)\n",
        "    is_debug = configs.get('is_debug', False)\n",
        "    dataset = configs.get('DATASET')\n",
        "    is_generate_csvs = configs.get('is_generate_csvs', False)\n",
        "\n",
        "    # Use the values in your experiments\n",
        "    logger.info(f\"{dataset}: Running experiment for ID {id} with seed {seed}\")\n",
        "\n",
        "    ### ETTh data 720_96 config:\n",
        "    columns = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']\n",
        "    T = configs.get('T', 2785)\n",
        "    L = configs.get('L', 720)\n",
        "    H = configs.get('H', 96)\n",
        "    C = len(columns)\n",
        "\n",
        "    logger.info(f\"Configs: T={T}, lambda_val={lambda_val}, delta_val={delta_val}\")\n",
        "    ## 1. Setup I/O: Extract True Label and N Expert Predictions\n",
        "    if is_generate_csvs:\n",
        "        df_Y, expert_df_X_dict, X, Y = um.setup_moef_io_all_experts(seed=seed, **configs)\n",
        "        # Should get back Y: (2785, 96, 7) and X_old: (3, 2785, 96, 7) -> X_permuted: (2785, 3, 96, 7)\n",
        "    else:\n",
        "        # Load X, Ys direclty from the test_results folder\n",
        "        X, Y, expert_names, expert_metrics = um.get_moef_io_for_all_experts_from_test_results(seed=seed, **configs)\n",
        "\n",
        "    ## 2. Setup Experts, Loss Function and Init Variables\n",
        "    num_of_experts = N = X.shape[1]\n",
        "    X = X.reshape(T, N, H*C)\n",
        "    Y = Y.reshape(T, H*C)\n",
        "\n",
        "    def make_expert(i):\n",
        "        return lambda x: x[-1, i, :] if x.shape[0] > 0 else torch.randn(H * C)\n",
        "    experts = [make_expert(i) for i in range(num_of_experts)]\n",
        "\n",
        "    ## 3. Call MoE-F ALGORITHM with Setup\n",
        "    Y_hat, prob_Y_hat, Q, pi, pi_bar_list = mixture_of_filters(X, Y, seed=seed,\n",
        "                                                               experts=experts, lam=lambda_val, delta=delta_val,\n",
        "                                                               **configs)\n",
        "    if torch.isnan(Y_hat).any():\n",
        "        logger.warning(f\"Warning: NaN values found in Y_hat with window size T = {T}.\")\n",
        "        logger.warning(\"Existing program - as going forward is pointless\")\n",
        "        raise ValueError(\"NaN values detected in Y_hat tensor.\")\n",
        "    else:\n",
        "        logger.info(f\"Yay! No NaN values in Y_hat with window size T = {T}\")\n",
        "\n",
        "    # Calculate performance\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    loss_mse = loss_fn(Y_hat, Y)\n",
        "    loss_mae = torch.mean(torch.abs(Y_hat - Y))\n",
        "    loss_rse = torch.sqrt(torch.sum((Y - Y_hat) ** 2)) / torch.sqrt(torch.sum((Y - Y.mean()) ** 2))\n",
        "\n",
        "    logger.info(f\"MoE-F Loss MSE: {loss_mse}, MAE: {loss_mae}, RSE: {loss_rse}\")\n",
        "    logger.info(\"-\" * 20)\n",
        "    for k,v in expert_metrics.items():\n",
        "        # expert_metrics[expert_name] = (mse, mae, rse)\n",
        "        mse, mae, rse = v\n",
        "        logger.info(f\"\\t{k} Loss MSE: {mse}, MAE: {mae}, RSE: {rse}\")\n",
        "\n",
        "    if is_save_results:\n",
        "        ## Save the results in \"experiments_tsf/ETTh1_sl720_pl96/all-experts_seed-0/df_moef.csv\"\n",
        "        results_save_dir = configs.get(\"EXPERIMENTS_PATH\")\n",
        "        results_dir = os.path.join(results_save_dir, f\"{id}_seed-{seed}\")\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        result_file_path = os.path.join(results_dir, \"result.txt\")\n",
        "        logger.info(f\"Saving results to {result_file_path}\")\n",
        "        with open(result_file_path, mode='a') as result_file:\n",
        "            result_file.write(\"\\n\")\n",
        "            result_file.write(f\"{dataset}_{L}_{H}_MoE-F_test_0_seed{seed}_lambda-{lambda_val}_alpha-{alpha_val}_delta-{delta_val}\\n\")\n",
        "            result_file.write(\"\\n\")\n",
        "            result_file.write(f\"Experiment ID: {id}, Seed: {seed}, Lambda: {lambda_val}, Alpha: {alpha_val}, Delta: {delta_val}\\n\")\n",
        "            result_file.write( f\"MoE-F MSE: {loss_mse.item():.4f}, MAE: {loss_mae.item():.4f}, RSE: {loss_rse.item():.4f}\\n\")\n",
        "            result_file.write(\"\\n\")\n",
        "\n",
        "            # Log and save expert metrics\n",
        "            result_file.write(\"Expert Models Performance:\\n\")\n",
        "            result_file.write(\"-\" * 25 + \"\\n\")\n",
        "            for expert_name, (mse, mae, rse) in expert_metrics.items():\n",
        "                result_file.write(f\"{expert_name}\\t MSE: {mse:.4f}, MAE: {mae:.4f}, RSE: {rse:.4f}\\n\")\n",
        "\n",
        "            # Additional content formatting\n",
        "            #result_file.write(f\"mse:{loss_mse.item():.4f}, mae:{loss_mae.item():.4f}, rse:{loss_rse.item():.4f}\\n\")\n",
        "            result_file.write(\"\\n\")\n",
        "            result_file.write(\"=\"*100 + \"\\n\")\n",
        "\n",
        "        logger.info(f\"Results saved to {result_file_path}\")\n",
        "    logger.info(\"=\" * 50)\n",
        "\n",
        "def main(config_file: str = \"config_720_96.yaml\"):\n",
        "    configs = load_config(config_file)\n",
        "\n",
        "    # List of IDs to iterate through\n",
        "    IDs = [\"all-experts\"]\n",
        "    SEEDS = configs['SEEDS']\n",
        "\n",
        "    # Run experiments\n",
        "    for id in IDs:\n",
        "        logger.info(\"=\" * 50)\n",
        "        logger.info(f\"Running for: {id} ... \")\n",
        "        for seed in SEEDS[:1]:\n",
        "            logger.info(f\"Using seed: {seed}\")\n",
        "            torch.manual_seed(seed)\n",
        "            run_experiments_data(id=id, seed=seed, **configs)\n",
        "        logger.info(\"=\" * 50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fire.Fire(main)"
      ]
    }
  ]
}